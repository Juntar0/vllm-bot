#!/usr/bin/env python3
"""
Test vLLM Connection
"""
import json
import sys
import requests
from pathlib import Path


def load_config():
    """Load config.json"""
    config_path = Path("config/config.json")
    
    if not config_path.exists():
        print("âŒ config/config.json not found")
        print("ğŸ’¡ Run: cp config/config.cli.json config/config.json")
        return None
    
    with open(config_path) as f:
        return json.load(f)


def test_connection(base_url, model):
    """Test vLLM API connection"""
    url = f"{base_url.rstrip('/')}/chat/completions"
    
    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": "Hello! Say hi in one word."}
        ],
        "max_tokens": 10
    }
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer dummy"
    }
    
    try:
        print(f"ğŸ“¡ Testing connection to: {url}")
        print(f"ğŸ¤– Model: {model}")
        print()
        
        response = requests.post(url, json=payload, headers=headers, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        message = data["choices"][0]["message"]["content"]
        
        print("âœ… Connection successful!")
        print(f"ğŸ“ Response: {message}")
        print()
        return True
        
    except requests.exceptions.ConnectionError:
        print("âŒ Connection failed: Cannot reach vLLM server")
        print(f"ğŸ’¡ Make sure vLLM is running at {base_url}")
        return False
        
    except requests.exceptions.Timeout:
        print("âŒ Connection timeout")
        return False
        
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTP error: {e}")
        if response.status_code == 404:
            print(f"ğŸ’¡ Model '{model}' not found on server")
        return False
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False


def main():
    print("ğŸ§ª vLLM Connection Test")
    print("=" * 50)
    print()
    
    config = load_config()
    if not config:
        sys.exit(1)
    
    vllm_config = config.get("vllm", {})
    base_url = vllm_config.get("base_url")
    
    if not base_url:
        print("âŒ vllm.base_url not configured")
        sys.exit(1)
    
    # Get available models from config
    models = vllm_config.get("available_models", ["gpt-oss-medium"])
    
    print(f"Testing {len(models)} available model(s):")
    print()
    
    results = {}
    for model in models:
        print(f"--- {model} ---")
        results[model] = test_connection(base_url, model)
        print()
    
    # Summary
    print("=" * 50)
    print("ğŸ“Š Summary:")
    print()
    for model, success in results.items():
        status = "âœ…" if success else "âŒ"
        print(f"  {status} {model}")
    print()
    
    if all(results.values()):
        print("ğŸ‰ All models are accessible!")
        print("âœ… You can now run: python cli.py")
    else:
        print("âš ï¸  Some models are not accessible")
        print("ğŸ’¡ Check vLLM server configuration")


if __name__ == "__main__":
    main()
